# Poly-HumanEval
Artifact for ICSE 2025 Submission: "Unraveling the Potential of Large Language Models in Code Translation: How Far Are We?"

## Benchmark

We conduct the **PolyHumanEval** benchmark, a variant of [`OpenAI HumanEval`](https://github.com/openai/human-eval) that support 14 programming languages: C++, C# , Dart, Go, Java, JavaScript, Kotlin, PHP, Python, Ruby, Rust, Scala, Swift and TypeScript. 

## TestDSL 
The **PolyHumanEval** benchmark is described in `TestDSL` format, which 

For example, the `has_close_element` in HumanEval:
```python
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    …
assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True
assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False
```
is describe by this `TestDSL` code:
```testdsl
code {
    func has_close_elements(numbers: list<double>, threshold: double) -> bool
}
tests {
    template nse {
        ([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) -> true
        ([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) -> false
    …   
    }
}
```
The whole benchmark data is shown in [`benchmark/poly_humaneval.testdsl`](benchmark/poly_humaneval.testdsl), which describes function signature and testcases for all 164 questions in HumanEval. Our handcraft solutions (also used for source code of translation) are shown in [`benchmark/poly_humaneval_sol.json`](benchmark/poly_humaneval_sol.json).

## Test Generation & Evaluation Tool

We then develop a tool for parsing the `TestDSL` data, generating test programs in all 14 programming languages, and executing the test programs to get the results. They are placed in the [`evaluation`](evaluation) folder. An example of usage is shown in [`evaluation/code/example.py`](evaluation/code/example.py).

For easier reproduction, we use [`Nix`](https://github.com/NixOS/nix) to manage the dependencies in [`evaluation/shell.nix`](evaluation/shell.nix).

Following these steps to reproducible the runtime environment:
```bash
> git clone https://github.com/AnonymousUser257/poly-humaneval
> cd evaluation
> nix-shell --pure
```

If the environment is well-settled, you can evaluate all the gold solutions by running the python script:
```bash
> python code/check_gold_solution_parallel.py
```
Which should not print any failed message.

Then, you can evaluate translation results generated by LLMs:
```bash
> mkdir ./output
> python code/check_generated_parallel --input data/RQ1-2_CodeLlama-13B.json --output output/evaluate_result.json
> python code/calculate_ca.py --input output/evaluate_result.json --output output/ca_result.json
```

## Other Artifacts

All the LLM-generated results in our experiments are placed in [llm_generated_translations](llm_generated_translations).

The artifacts for Self-Training are placed in [self_tranining_data](self_tranining_data). 
- [self_tranining_data/generated_py_codes.json](self_tranining_data/generated_py_codes.json) is the generated Python code with CodeLlama-13B.
- [self_tranining_data/generated.testdsl](self_tranining_data/generated_py_codes.json) is the generated Python testcases converted to `TestDSL` format.
- [self_tranining_data/fine-tune-prompts](self_tranining_data/generated_py_codes.json) is the Python-Go parallel data for fine-tuning CodeLlama-13B.
- [self_tranining_data/fine-tune-lora-checkpoints](self_tranining_data/generated_py_codes.json) is the LoRA adapter model checkpoints after self-training.