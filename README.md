# Poly-HumanEval
Artifact for ICSE 2025 Submission: "Unraveling the Potential of Large Language Models in Code Translation: How Far Are We?"

# What is in this repository?
1.  PolyHumanEval benchmark include description in TestDSL format and solution on all 14 programming languages([./benchmark](benchmark/README.md)).
2.  All LLM-generated translation results use in our study([./llm_generated_translation](llm_generated_translation/README.md)).
3.  The generated data and lora model checkpoints in Self-Training([./self_traininig_data](self_traininig_data/README.md)).
4.  Evaluation tool for generate and execute test programs to get evaluation results([./evaluation](evaluation/README.md)).
